{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31eee5c8-cf80-4208-8886-5ac48db4de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "656417ab-4765-4b88-ac09-4ae055796bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", None)\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3f319d-e7c8-4e76-99bd-9e5e37ec7a00",
   "metadata": {},
   "source": [
    "# Scraping/Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34f966a1-1528-4496-8012-c340b7e902d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Any\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "import re\n",
    "\n",
    "from haystack.dataclasses import ByteStream, Document\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "    \n",
    "\n",
    "class Scraper():\n",
    "    \"\"\"\n",
    "    A class used to scrap URLs from *.admin.ch websites.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    fetcher : LinkContentFetcher\n",
    "        An instance of LinkContentFetcher to fetch the content of URLs.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    scrap_urls(url_list: List[str]) -> List[ByteStream]\n",
    "        Scrapes the given URLs and returns the content as a list of ByteStreams.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.fetcher = LinkContentFetcher()\n",
    "\n",
    "    def scrap_urls(self, urls: List[str]) -> List[ByteStream]:\n",
    "        \"\"\"\n",
    "        Scrapes the given URLs and returns the content as a list of ByteStreams.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        urls : List[str]\n",
    "            A list of URLs to scrape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[ByteStream]\n",
    "            A list of ByteStreams containing the content of the scraped URLs.\n",
    "        \"\"\"\n",
    "\n",
    "        streams = self.fetcher.run(urls=urls)\n",
    "        return streams[\"streams\"]\n",
    "\n",
    "    async def fetch(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Fetches the content from a given URL.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url : str\n",
    "            The URL to fetch content from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bytes\n",
    "            The content of the URL.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        aiohttp.ClientError\n",
    "            If the fetch operation fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            async with aiohttp.ClientSession(trust_env=True) as session:\n",
    "                async with session.get(url, timeout=10) as response:\n",
    "                    response.raise_for_status()\n",
    "                    return await response.text()\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(url, \"----\", e)\n",
    "\n",
    "class Parser():\n",
    "    \"\"\"\n",
    "    A class used to parse and clean documents.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    pdf_converter : PyPDFToDocument\n",
    "        An instance of PyPDFToDocument to convert PDF content to Document objects.\n",
    "\n",
    "    cleaner : DocumentCleaner\n",
    "        An instance of DocumentCleaner to clean documents.\n",
    "\n",
    "    splitter : DocumentSplitter\n",
    "        An instance of DocumentSplitter to split documents into chunks.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    parse_html(html: bytes) -> List[str]\n",
    "        Extracts URLs from the given HTML content.\n",
    "\n",
    "    convert_pdf_to_documents(content: List[ByteStream]) -> List[Document]\n",
    "        Converts PDF content to Document objects.\n",
    "\n",
    "    clean_documents(documents: List[Document]) -> List[Document]\n",
    "        Cleans the given documents.\n",
    "\n",
    "    split_documents(documents: List[Document]) -> List[Document]\n",
    "        Splits the given documents into chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cleaner = DocumentCleaner(\n",
    "            remove_empty_lines=True,\n",
    "            remove_extra_whitespaces=True,\n",
    "            remove_repeated_substrings=False,\n",
    "        )\n",
    "        self.splitter = DocumentSplitter(\n",
    "            split_by=\"sentence\",\n",
    "            split_length=5,\n",
    "            split_overlap=1,\n",
    "            split_threshold=4,\n",
    "        )\n",
    "\n",
    "    def remove_empty_documents(self, documents: List[Any]) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Remove documents from the list that have their data attribute set to None.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents : list\n",
    "            Document objects to be filtered.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            Document objects where the content attribute is not None.\n",
    "        \"\"\"\n",
    "        return [doc for doc in documents if doc.content is not None]\n",
    "\n",
    "    def remove_duplicate_links(self, links):\n",
    "        \"\"\"\n",
    "        Removes duplicate links from a list of tags.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        links : list of bs4.element.Tag\n",
    "            The list of tags to remove duplicates from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of bs4.element.Tag\n",
    "            The list of tags without duplicates.\n",
    "        \"\"\"\n",
    "        seen_hrefs = set()\n",
    "        unique_tags = []\n",
    "        for tag in links:\n",
    "            href = tag[\"href\"]\n",
    "            if href not in seen_hrefs:\n",
    "                seen_hrefs.add(href)\n",
    "                unique_tags.append(tag)\n",
    "        return unique_tags\n",
    "\n",
    "    def clean_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Removes docs with None content and cleans the given documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents : List[Document]\n",
    "            The documents to clean.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Document]\n",
    "            Cleaned documents.\n",
    "        \"\"\"\n",
    "        return self.cleaner.run(documents=documents)\n",
    "\n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Removes docs with None content and splits the given documents into chunks.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents : List[Document]\n",
    "            The documents to split into chunks.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Document]\n",
    "            A list of documents split into chunks.\n",
    "        \"\"\"\n",
    "        return self.splitter.run(documents=documents)\n",
    "\n",
    "    def contains_tag(self, tag):\n",
    "        \"\"\"\n",
    "        Checks if a tag contains a memento URL.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tag : bs4.element.Tag\n",
    "            The tag to check.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if the tag contains a memento URL, False otherwise.\n",
    "        \"\"\"\n",
    "        if tag.name == \"a\" and \"href\" in tag.attrs:\n",
    "            href = tag[\"href\"]\n",
    "            decoded_href = unquote(href)\n",
    "            keywords = [\"Merkblätter/\", \"Mémentos/\", \"Opuscoli\", \"Leaflets/\"]\n",
    "            return any(keyword in decoded_href for keyword in keywords)\n",
    "        return False\n",
    "\n",
    "    def get_pdf_paths(self, soup):\n",
    "        \"\"\"\n",
    "        Extracts the paths of PDF documents from a BeautifulSoup object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        soup : BeautifulSoup\n",
    "            The BeautifulSoup object to extract PDF paths from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of str\n",
    "            The list of PDF paths.\n",
    "        \"\"\"\n",
    "\n",
    "        pdf_paths = [\n",
    "            a[\"href\"]\n",
    "            for a in soup.find_all(\"a\", {\"class\": \"co-document-content\"})\n",
    "            if \"/p/\" in a[\"href\"]\n",
    "        ]\n",
    "        return pdf_paths\n",
    "\n",
    "    def get_pdf_metadata(self, soup):\n",
    "        \"\"\"\n",
    "        Extracts the metadata of PDF documents from a BeautifulSoup object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        soup : BeautifulSoup\n",
    "            The BeautifulSoup object to extract PDF paths from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of str\n",
    "            The list of PDF paths.\n",
    "        \"\"\"\n",
    "        pdfs = soup.find_all(\"div\", {\"class\": \"sc-element co-fileType-PDF published\"})\n",
    "        \n",
    "        pdf_metadata = [\n",
    "            {\n",
    "                \"title\": pdf.find(\"div\", {\"class\": \"co-document-main\"}).find(\"b\").text.strip(),\n",
    "                \"url\": (a := pdf.find(\"a\")) and (h := a.get(\"href\", \"\")) and \"/p/\" in h and \"https://ahv-iv.ch\" + h or None,\n",
    "                \"last_modification\": extract_date_from_str(pdf.find(\"p\", {\"class\": \"co-document-infos\"}).text.strip()),\n",
    "                \"state\": extract_date_from_str(pdf.find(\"div\", {\"class\": \"co-document-state\"}).text.strip()),\n",
    "            } for pdf in pdfs\n",
    "        ]\n",
    "        return pdf_metadata\n",
    "\n",
    "    def parse_urls(self, content: str) -> List[str]:\n",
    "        soup = BeautifulSoup(content, features=\"html.parser\")\n",
    "\n",
    "        # Find all \"a\" tags with href containing \"Merkblätter/Mémentos/Opuscoli/Leaflets\" (and subsequent path)\n",
    "        links = soup.find_all(self.contains_tag)\n",
    "        links = self.remove_duplicate_links(links)\n",
    "\n",
    "        url_list = [link[\"href\"] for link in links]\n",
    "\n",
    "        return url_list\n",
    "\n",
    "    def convert_to_documents(self, content: List[Any]) -> List[Any]:\n",
    "        return PyPDFToDocument().run(sources=content)\n",
    "\n",
    "scraper = Scraper()\n",
    "parser = Parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4cc94d-1f5a-4458-bac5-30bfd4982390",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_pages_from_sitemap(\n",
    "    scraper: Any, parser: Any, sitemap_url: str\n",
    ") -> List[ByteStream]:\n",
    "    # Get sitemap\n",
    "    sitemap = await scraper.fetch(sitemap_url)\n",
    "\n",
    "    # Extract URLs from sitemap\n",
    "    url_list = parser.parse_urls(sitemap)\n",
    "\n",
    "    # Get HTML from each URL\n",
    "    return scraper.scrap_urls(url_list)\n",
    "\n",
    "\n",
    "async def from_pages_to_content(\n",
    "    scraper: Any, parser: Any, pages: List[ByteStream]\n",
    ") -> List[Any]:\n",
    "    soups = []\n",
    "    for page in pages:\n",
    "        soups.append(BeautifulSoup(page.data, features=\"html.parser\"))\n",
    "\n",
    "    # Get PDF paths from each memento section\n",
    "    pdf_metadata = []\n",
    "    for soup in soups:\n",
    "        metadata = parser.get_pdf_metadata(soup)\n",
    "        pdf_metadata.extend(metadata)\n",
    "\n",
    "    # PROCESSING URLS\n",
    "\n",
    "    # Scrap PDFs from each memento section\n",
    "    #pdf_urls = [\"https://ahv-iv.ch\" + pdf_path for pdf_path in pdf_paths]\n",
    "    pdf_urls = [m[\"url\"] for m in pdf_metadata]\n",
    "\n",
    "    # Add \"it\", \"fr\" pdf paths\n",
    "    #pdf_urls.extend([pdf_url.replace(\".d\", \".f\") for pdf_url in pdf_urls])\n",
    "    #pdf_urls.extend([pdf_url.replace(\".d\", \".i\") for pdf_url in pdf_urls])\n",
    "\n",
    "    return pdf_metadata, scraper.scrap_urls(pdf_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da8c99f-2011-4f2d-94bb-8526c2a13d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pdf(content: bytes) -> bool:\n",
    "    \"\"\"Returns True if content is a valid PDF (starts with %PDF-), else False.\"\"\"\n",
    "    return content.startswith(b'%PDF-')\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "def validate_urls_and_contents(metadatas: List[str], contents: List[bytes], lang: str) -> Tuple[List[str], List[bytes]]:\n",
    "    \"\"\"\n",
    "    Filters urls in metadatas and contents by language suffix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metadatas : List[str]\n",
    "        List of URLs in metadatas with language suffixes (e.g., .de, .fr).\n",
    "    contents : List[bytes]\n",
    "        List of corresponding content bytes.\n",
    "    lang : str\n",
    "        The language suffix to match (e.g., 'de').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[str], List[bytes]]\n",
    "        Filtered urls and contents, aligned by index.\n",
    "    \"\"\"\n",
    "    filtered_metadatas = []\n",
    "    filtered_contents = []\n",
    "\n",
    "    for metadata, content in zip(metadatas, contents):\n",
    "        if metadata[\"url\"].split(\".\")[-1] == lang:\n",
    "            filtered_metadatas.append(metadata)\n",
    "            filtered_contents.append(content)\n",
    "\n",
    "    return filtered_metadatas, filtered_contents\n",
    "\n",
    "def extract_date_from_str(string: str):\n",
    "    \"\"\"Returns a string with extract date in format dd.mm.yyyy\"\"\"\n",
    "    pattern = r'\\b\\d{2}\\.\\d{2}\\.\\d{4}\\b'\n",
    "\n",
    "    try:\n",
    "        return re.findall(pattern, string)[0]\n",
    "    except Exception as e:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d91a2401-a5bf-41dd-92b2-802319dc9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitemaps = {\n",
    "    \"d\": \"https://www.ahv-iv.ch/de/Sitemap-DE\",\n",
    "    \"f\": \"https://www.ahv-iv.ch/fr/Sitemap-FR\",\n",
    "    \"i\": \"https://www.ahv-iv.ch/it/Sitemap-IT\",\n",
    "    \"e\": \"https://www.ahv-iv.ch/en/Sitemap-EN\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c473c7d-37f8-4fb9-b56c-98b1f1e48492",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"../pdfs/\"\n",
    "\n",
    "for lang, sitemap in sitemaps.items():\n",
    "    \n",
    "    pages = await get_pages_from_sitemap(\n",
    "        scraper=scraper,\n",
    "        parser=parser,\n",
    "        sitemap_url=sitemap\n",
    "    )\n",
    "\n",
    "    metadatas, contents = await from_pages_to_content(\n",
    "        scraper=scraper,\n",
    "        parser=parser,\n",
    "        pages=pages\n",
    "    )\n",
    "\n",
    "    valid_metadatas, valid_contents = validate_urls_and_contents(metadatas, contents, lang)\n",
    "    \n",
    "    for metadata, content in zip(valid_metadatas, valid_contents):\n",
    "\n",
    "        if is_pdf(content.data):\n",
    "            filename = metadata[\"url\"].split(\"/\")[-1].replace(\".\", \"_\") + \".pdf\"\n",
    "            os.makedirs(os.path.join(SAVE_PATH, lang), exist_ok=True)\n",
    "            \n",
    "            with open(os.path.join(SAVE_PATH, lang, filename), \"wb\") as f:\n",
    "                f.write(content.data)\n",
    "\n",
    "    final_metadata = {}\n",
    "    for meta in valid_metadatas:\n",
    "        pdf_id = meta[\"url\"].split(\"/\")[-1].replace(\".\", \"_\")\n",
    "        final_metadata[pdf_id] = meta\n",
    "    \n",
    "    with open(os.path.join(SAVE_PATH, lang, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(final_metadata, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc14534-dfc2-4a84-8acf-759a31425dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_data_prep",
   "language": "python",
   "name": "venv_data_prep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
